{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST in Pyro.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasadshreyas/CS7290-Causal-ML-PyTorch/blob/main/MNIST_in_Pyro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyro-ppl "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLewIjHyhQRZ",
        "outputId": "c2107709-3219-485d-c218-96bb081277c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyro-ppl\n",
            "  Downloading pyro_ppl-1.8.1-py3-none-any.whl (718 kB)\n",
            "\u001b[K     |████████████████████████████████| 718 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (4.64.0)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (3.3.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from pyro-ppl) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.11.0->pyro-ppl) (4.2.0)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Only pixels\n",
        "- Condition on the labels (Digits themselves)\n",
        "- Labels are getting encoded into z variables\n",
        "\n",
        "Encoder\n",
        "digits = pyro.sample( categorical, obs = (digits))\n",
        "\n",
        "\n",
        "Decoder\n",
        "modifed d enconder\n",
        "\n",
        "\n",
        "- T mnist\n",
        "- typeface\n"
      ],
      "metadata": {
        "id": "2ylAJGxFlpUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_AX7cWnWlpHN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J0VP4RB6c-ZR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from pyro.contrib.examples.util import MNIST\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.contrib.examples.util  # patches torchvision\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert pyro.__version__.startswith('1.8.1')\n",
        "pyro.distributions.enable_validation(False)\n",
        "pyro.set_rng_seed(0)\n",
        "# Enable smoke test - run the notebook cells on CI.\n",
        "smoke_test = 'CI' in os.environ"
      ],
      "metadata": {
        "id": "nAWaRQtTddKb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for loading and batching MNIST dataset\n",
        "def setup_data_loaders(batch_size=128, use_cuda=False):\n",
        "    root = './data'\n",
        "    download = True\n",
        "    trans = transforms.ToTensor()\n",
        "    train_set = MNIST(root=root, train=True, transform=trans,\n",
        "                      download=download)\n",
        "    test_set = MNIST(root=root, train=False, transform=trans)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "        batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
        "        batch_size=batch_size, shuffle=False, **kwargs)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "GSc5osDXdjU_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # setup the two linear transformations used\n",
        "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
        "        self.fc21 = nn.Linear(hidden_dim, 784)\n",
        "        # setup the non-linearities\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z):\n",
        "        # define the forward computation on the latent z\n",
        "        # first compute the hidden units\n",
        "        hidden = self.softplus(self.fc1(z))\n",
        "        # return the parameter for the output Bernoulli\n",
        "        # each is of size batch_size x 784\n",
        "        loc_img = self.sigmoid(self.fc21(hidden))\n",
        "        return loc_img\n",
        "\n"
      ],
      "metadata": {
        "id": "4GkMPwGTdmNw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        # setup the three linear transformations used\n",
        "        self.fc1 = nn.Linear(784, hidden_dim)\n",
        "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
        "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
        "        # setup the non-linearities\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define the forward computation on the image x\n",
        "        # first shape the mini-batch to have pixels in the rightmost dimension\n",
        "        x = x.reshape(-1, 784)\n",
        "        # then compute the hidden units\n",
        "        hidden = self.softplus(self.fc1(x))\n",
        "        # then return a mean vector and a (positive) square root covariance\n",
        "        # each of size batch_size x z_dim\n",
        "        z_loc = self.fc21(hidden)\n",
        "        z_scale = torch.exp(self.fc22(hidden))\n",
        "        return z_loc, z_scale"
      ],
      "metadata": {
        "id": "obm6Khmzf69d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model p(x|z)p(z)\n",
        "def model(self, x):\n",
        "    # register PyTorch module `decoder` with Pyro\n",
        "    pyro.module(\"decoder\", self.decoder)\n",
        "    with pyro.plate(\"data\", x.shape[0]):\n",
        "        # setup hyperparameters for prior p(z)\n",
        "        z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
        "        z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
        "        # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "        # decode the latent code z\n",
        "        loc_img = self.decoder(z)\n",
        "        # score against actual images\n",
        "        pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))"
      ],
      "metadata": {
        "id": "aW_OI3PhgiA1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the guide (i.e. variational distribution) q(z|x)\n",
        "def guide(self, x):\n",
        "    # register PyTorch module `encoder` with Pyro\n",
        "    pyro.module(\"encoder\", self.encoder)\n",
        "    with pyro.plate(\"data\", x.shape[0]):\n",
        "        # use the encoder to get the parameters used to define q(z|x)\n",
        "        z_loc, z_scale = self.encoder(x)\n",
        "        # sample the latent code z\n",
        "        pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))"
      ],
      "metadata": {
        "id": "Zcvsk-3ugnE2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    # by default our latent space is 50-dimensional\n",
        "    # and we use 400 hidden units\n",
        "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
        "        super().__init__()\n",
        "        # create the encoder and decoder networks\n",
        "        self.encoder = Encoder(z_dim, hidden_dim)\n",
        "        self.decoder = Decoder(z_dim, hidden_dim)\n",
        "\n",
        "        if use_cuda:\n",
        "            # calling cuda() here will put all the parameters of\n",
        "            # the encoder and decoder networks into gpu memory\n",
        "            self.cuda()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "    # define the model p(x|z)p(z)\n",
        "    def model(self, x):\n",
        "        # register PyTorch module `decoder` with Pyro\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # setup hyperparameters for prior p(z)\n",
        "            z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
        "            z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
        "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "            # decode the latent code z\n",
        "            loc_img = self.decoder(z)\n",
        "            # score against actual images\n",
        "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
        "\n",
        "    # define the guide (i.e. variational distribution) q(z|x)\n",
        "    def guide(self, x):\n",
        "        # register PyTorch module `encoder` with Pyro\n",
        "        pyro.module(\"encoder\", self.encoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # use the encoder to get the parameters used to define q(z|x)\n",
        "            z_loc, z_scale = self.encoder(x)\n",
        "            # sample the latent code z\n",
        "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "\n",
        "    # define a helper function for reconstructing images\n",
        "    def reconstruct_img(self, x):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.encoder(x)\n",
        "        # sample in latent space\n",
        "        z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        loc_img = self.decoder(z)\n",
        "        return loc_img\n"
      ],
      "metadata": {
        "id": "5mdyPPmQgsm3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = VAE()\n",
        "optimizer = Adam({\"lr\": 1.0e-3})\n",
        "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())"
      ],
      "metadata": {
        "id": "gfdbMULGgynC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(svi, train_loader, use_cuda=False):\n",
        "    # initialize loss accumulator\n",
        "    epoch_loss = 0.\n",
        "    # do a training epoch over each mini-batch x returned\n",
        "    # by the data loader\n",
        "    for x, _ in train_loader:\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "        # do ELBO gradient and accumulate loss\n",
        "        epoch_loss += svi.step(x)\n",
        "\n",
        "    # return epoch loss\n",
        "    normalizer_train = len(train_loader.dataset)\n",
        "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
        "    return total_epoch_loss_train\n",
        "\n",
        "def evaluate(svi, test_loader, use_cuda=False):\n",
        "    # initialize loss accumulator\n",
        "    test_loss = 0.\n",
        "    # compute the loss over the entire test set\n",
        "    for x, _ in test_loader:\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "        # compute ELBO estimate and accumulate loss\n",
        "        test_loss += svi.evaluate_loss(x)\n",
        "    normalizer_test = len(test_loader.dataset)\n",
        "    total_epoch_loss_test = test_loss / normalizer_test\n",
        "    return total_epoch_loss_test"
      ],
      "metadata": {
        "id": "ShezedYSg96B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run options\n",
        "LEARNING_RATE = 1.0e-3\n",
        "USE_CUDA = False\n",
        "\n",
        "# Run only for a single iteration for testing\n",
        "NUM_EPOCHS = 1 if smoke_test else 10\n",
        "TEST_FREQUENCY = 5"
      ],
      "metadata": {
        "id": "egbfhRLNg-8d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
        "\n",
        "# clear param store\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# setup the VAE\n",
        "vae = VAE(use_cuda=USE_CUDA)\n",
        "\n",
        "# setup the optimizer\n",
        "adam_args = {\"lr\": LEARNING_RATE}\n",
        "optimizer = Adam(adam_args)\n",
        "\n",
        "# setup the inference algorithm\n",
        "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
        "\n",
        "train_elbo = []\n",
        "test_elbo = []\n",
        "# training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
        "    train_elbo.append(-total_epoch_loss_train)\n",
        "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
        "\n",
        "    if epoch % TEST_FREQUENCY == 0:\n",
        "        # report test diagnostics\n",
        "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
        "        test_elbo.append(-total_epoch_loss_test)\n",
        "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BnACHvKhGzD",
        "outputId": "92802ba4-52fa-4b02-a939-524baba2d2d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 000]  average training loss: 191.1242\n",
            "[epoch 000] average test loss: 156.7098\n",
            "[epoch 001]  average training loss: 147.1483\n",
            "[epoch 002]  average training loss: 133.9980\n",
            "[epoch 003]  average training loss: 125.6235\n",
            "[epoch 004]  average training loss: 120.1644\n",
            "[epoch 005]  average training loss: 116.6344\n",
            "[epoch 005] average test loss: 114.5177\n",
            "[epoch 006]  average training loss: 114.1435\n",
            "[epoch 007]  average training loss: 112.3651\n",
            "[epoch 008]  average training loss: 110.9969\n",
            "[epoch 009]  average training loss: 109.9670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "40Xi0rHYzNPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}