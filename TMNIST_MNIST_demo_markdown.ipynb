{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TMNIST_MNIST_demo_markdown.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnmVGjaVI8ACmaaJCzFQHN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prasadshreyas/CS7290-Causal-ML-PyTorch/blob/main/TMNIST_MNIST_demo_markdown.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the libraries\n",
        "```python\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "from pyro.contrib.examples.util import MNIST\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.contrib.examples.util  \n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "assert pyro.__version__.startswith('1.8.1') # make sure we have Pyro 1.8.1\n",
        "pyro.distributions.enable_validation(False) \n",
        "pyro.set_rng_seed(0) # make results reproducible\n",
        "# Enable smoke test - run the notebook cells on CI.\n",
        "smoke_test = 'CI' in os.environ \n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "eh2STq5aDbZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A custom dataloader for T-MNIST Dataset. [link](https://raw.githubusercontent.com/prasadshreyas/CS7290-Causal-ML-PyTorch/main/data/TMNIST/TMNIST_Data.csv)\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "class TMNIST_MNIST_Dataset(Dataset):\n",
        "    '''\n",
        "    Dataset containing both MNIST and T-MNIST data.\n",
        "\n",
        "    Input: 'path/to/csv/file' (Required)\n",
        "    Output: torch.utils.data.Dataset\n",
        "    \n",
        "    csv file should have the following columns:\n",
        "    - label: 0-9\n",
        "    - image: flattened image (28x28)\n",
        "    - is_handwritten: 0 or 1\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, csv_file, root_dir='', transform=None, target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))):\n",
        "        '''\n",
        "        Args: csv_file (string): Path to the csv file with annotations.    \n",
        "        '''\n",
        "        self.tmnist_frame = pd.read_csv(csv_file) # read the csv file\n",
        "        self.transform = transform # transform the data\n",
        "        self.root_dir = root_dir \n",
        "        self.target_transform = target_transform # transform the target into one-hot vector of size 10\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tmnist_frame) # return the length of the dataset\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist() # if the index is tensor, convert it to list\n",
        "        \n",
        "        digits = self.tmnist_frame.iloc[idx, 2:-1] # get the digits from the csv file\n",
        "        digits = np.array([digits])/255 # normalize the digits\n",
        "        digits = digits.reshape(28,28) # reshape the digits to 28x28 as the original MNIST dataset does\n",
        "        digits = digits.astype('float32') \n",
        "        \n",
        "        label = self.tmnist_frame.iloc[idx , 1]\n",
        "        label = np.array([label]) # get the label from the csv file\n",
        "        label = label.astype('int')\n",
        "\n",
        "\n",
        "        # is_hand is a boolean variable which is True if the digit is hand written and False if it is not.\n",
        "        is_handwritten = self.tmnist_frame.iloc[idx , -1] # get the is_handwritten from the csv file\n",
        "        is_handwritten = np.array([is_handwritten]) # convert the is_handwritten to numpy array\n",
        "        is_handwritten = is_handwritten.astype('int') # convert the is_handwritten to int\n",
        "        \n",
        "        \n",
        "\n",
        "        if self.transform:\n",
        "            digits = self.transform(digits)\n",
        "        return digits, int(label.squeeze()), int(is_hand.squeeze()), int(is_handwritten.squeeze())  # return the digits, label and the handwritten status\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "7p01hX2LD2nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "# for loading and batching TMNIST-MNIST dataset\n",
        "def setup_data_loaders(batch_size=64, use_cuda=False): \n",
        "    root = './data'\n",
        "    download = True\n",
        "    trans = transforms.ToTensor()\n",
        "    \n",
        "    tmnist_dataset = TMNISTDataset(\"https://raw.githubusercontent.com/prasadshreyas/CS7290-Causal-ML-PyTorch/main/data/TMNIST/TMNIST_Data.csv\", transform= transforms.ToTensor() )\n",
        "    n = len(tmnist_dataset) # number of samples in the dataset\n",
        "\n",
        "    # Split the dataset into train and test\n",
        "    train_length = int(n*0.7) \n",
        "    test_length = n - train_length\n",
        "    train_set,test_set = torch.utils.data.random_split( dataset = tmnist_dataset, lengths = [train_length,test_length], generator=torch.Generator().manual_seed(42))\n",
        "    \n",
        "\n",
        "    # Create data loaders for train and test\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "        batch_size=batch_size, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
        "        batch_size=batch_size, shuffle=False, **kwargs)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "ymsbHwJFOZDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class VAE(nn.Module):\n",
        "    # by default our latent space is 50-dimensional\n",
        "    # and we use 400 hidden units\n",
        "    def __init__(self, z_dim=50, hidden_dim=400, use_cuda=False):\n",
        "        super().__init__()\n",
        "        # create the encoder and decoder networks\n",
        "        self.encoder = Encoder(z_dim, hidden_dim) \n",
        "        self.decoder = Decoder(z_dim, hidden_dim)\n",
        "\n",
        "        if use_cuda:\n",
        "            # calling cuda() here will put all the parameters of\n",
        "            # the encoder and decoder networks into gpu memory\n",
        "            self.cuda()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "# define the model p(x|z)p(z) and p(x|h)p(h) for handwritten digits\n",
        "def model(self, x):\n",
        "    # register PyTorch module `decoder` with Pyro\n",
        "    pyro.module(\"decoder\", self.decoder)\n",
        "    with pyro.plate(\"data\", x.shape[0]):\n",
        "        # setup hyperparameters for prior p(z)\n",
        "        z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
        "        z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
        "        # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "\n",
        "        h_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
        "        h_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
        "        # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "        h = pyro.sample(\"hidden\", dist.Normal(h_loc, h_scale).to_event(1))\n",
        "        # decode the latent code z\n",
        "        loc_img = self.decoder(z)\n",
        "        \n",
        "        # score against actual images\n",
        "        pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
        "        \n",
        "        # decode the latent code h\n",
        "        loc_img = self.decoder(h)\n",
        "        # score against actual images\n",
        "        pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.reshape(-1, 784))\n",
        "\n",
        "   \n",
        "    # define the guide (i.e. variational distribution) q(z|x) and q(h|x) for handwritten digits\n",
        "    def guide(self, x):\n",
        "    # register PyTorch module `encoder` with Pyro\n",
        "        pyro.module(\"encoder\", self.encoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "        \n",
        "            # use the encoder to get the parameters used to define q(z|x)\n",
        "            z_loc, z_scale = self.encoder(x)\n",
        "            # sample the latent code z\n",
        "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
        "            # use the encoder to get the parameters used to define q(h|x)\n",
        "            h_loc, h_scale = self.encoder(x)\n",
        "            # sample the latent code h\n",
        "            pyro.sample(\"hidden\", dist.Normal(h_loc, h_scale).to_event(1))\n",
        "\n",
        "    # define a helper function for reconstructing images\n",
        "    def reconstruct_img(self, x):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.encoder(x)\n",
        "        # sample in latent space\n",
        "        z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        loc_img = self.decoder(z)\n",
        "        return loc_img\n",
        "```"
      ],
      "metadata": {
        "id": "iAD-xzrtOY5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "def train(svi, train_loader, use_cuda=False):\n",
        "    # initialize loss accumulator\n",
        "    epoch_loss = 0.\n",
        "    # do a training epoch over each mini-batch x returned\n",
        "    # by the data loader\n",
        "    for x, label, hand in train_loader:\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "        # do ELBO gradient and accumulate loss\n",
        "        epoch_loss += svi.step(x)\n",
        "\n",
        "    # return epoch loss\n",
        "    normalizer_train = len(train_loader.dataset)\n",
        "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
        "    return total_epoch_loss_train\n",
        "\n",
        "def evaluate(svi, test_loader, use_cuda=False):\n",
        "    # initialize loss accumulator\n",
        "    test_loss = 0.\n",
        "    # compute the loss over the entire test set\n",
        "    for x, label, hand  in test_loader:\n",
        "        # if on GPU put mini-batch into CUDA memory\n",
        "        if use_cuda:\n",
        "            x = x.cuda()\n",
        "        # compute ELBO estimate and accumulate loss\n",
        "        test_loss += svi.evaluate_loss(x)\n",
        "    normalizer_test = len(test_loader.dataset)\n",
        "    total_epoch_loss_test = test_loss / normalizer_test\n",
        "    return total_epoch_loss_test\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "_ZPgaJ12OY3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "# Run options\n",
        "LEARNING_RATE = 1.0e-3\n",
        "USE_CUDA = False\n",
        "\n",
        "# Run only for a single iteration for testing\n",
        "NUM_EPOCHS = 1 if smoke_test else 1\n",
        "TEST_FREQUENCY = 5\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "MqXlR6N0OYxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "train_loader, test_loader = setup_data_loaders(batch_size=256, use_cuda=USE_CUDA)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "JA8uYjaeO2fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "# clear param store\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# setup the VAE\n",
        "vae = VAE(use_cuda=USE_CUDA)\n",
        "\n",
        "# setup the optimizer\n",
        "adam_args = {\"lr\": LEARNING_RATE}\n",
        "optimizer = Adam(adam_args)\n",
        "\n",
        "# setup the inference algorithm\n",
        "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
        "```"
      ],
      "metadata": {
        "id": "NQu0dPiTO2cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "train_elbo = []\n",
        "test_elbo = []\n",
        "# training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
        "    train_elbo.append(-total_epoch_loss_train)\n",
        "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
        "\n",
        "    if epoch % TEST_FREQUENCY == 0:\n",
        "        # report test diagnostics\n",
        "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
        "        test_elbo.append(-total_epoch_loss_test)\n",
        "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "0yPKdaD-O2aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6BKsLWC5O2X4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynCbMJj_DK_u"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}